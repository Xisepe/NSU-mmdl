{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4XzNfVq5UeB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng(seed=42)\n",
        "class Example2D:\n",
        "    def __init__(self, x, y, label):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.label = label\n",
        "\n",
        "def classify_two_gauss_data(num_samples, noise):\n",
        "    points = []\n",
        "\n",
        "    variance_scale = np.vectorize(lambda x: 0.5 + 3.5 * (x / 0.5))\n",
        "    variance = variance_scale(noise)\n",
        "\n",
        "    def gen_gauss(cx, cy, label):\n",
        "        for _ in range(num_samples // 2):\n",
        "            x = rng.normal(cx, variance)\n",
        "            y = rng.normal(cy, variance)\n",
        "            points.append(Example2D(x, y, label))\n",
        "\n",
        "    gen_gauss(2, 2, 1)  # Gaussian with positive examples.\n",
        "    gen_gauss(-2, -2, 0)  # Gaussian with negative examples.\n",
        "    return points\n",
        "\n",
        "def classify_spiral_data(num_samples, noise):\n",
        "    points = []\n",
        "    n = num_samples // 2\n",
        "\n",
        "    def gen_spiral(delta_t, label):\n",
        "        for i in range(n):\n",
        "            r = i / n * 5\n",
        "            t = 1.75 * i / n * 2 * np.pi + delta_t\n",
        "            x = r * np.sin(t) + rng.uniform(-1, 1) * noise\n",
        "            y = r * np.cos(t) + rng.uniform(-1, 1) * noise\n",
        "            points.append(Example2D(x, y, label))\n",
        "\n",
        "    gen_spiral(0, 1)  # Positive examples.\n",
        "    gen_spiral(np.pi, 0)  # Negative examples.\n",
        "    return points\n",
        "\n",
        "def classify_xor_data(num_samples, noise):\n",
        "    def get_xor_label(p):\n",
        "        return 1 if p.x * p.y >= 0 else 0\n",
        "\n",
        "    points = []\n",
        "    for _ in range(num_samples):\n",
        "        x = rng.uniform(-5, 5)\n",
        "        padding = 0.3\n",
        "        x += padding if x > 0 else -padding  # Padding.\n",
        "        y = rng.uniform(-5, 5)\n",
        "        y += padding if y > 0 else -padding\n",
        "        noise_x = rng.uniform(-5, 5) * noise\n",
        "        noise_y = rng.uniform(-5, 5) * noise\n",
        "        label = get_xor_label(Example2D(x + noise_x, y + noise_y, None))\n",
        "        points.append(Example2D(x, y, label))\n",
        "    return points\n",
        "\n",
        "def classify_circle_data(num_samples, noise):\n",
        "    points = []\n",
        "    radius = 5\n",
        "\n",
        "    def get_circle_label(p, center):\n",
        "        return 1 if np.linalg.norm([p.x - center.x, p.y - center.y]) < (radius * 0.5) else 0\n",
        "\n",
        "    # Generate positive points inside the circle.\n",
        "    for _ in range(num_samples // 2):\n",
        "        r = rng.uniform(0, radius * 0.5)\n",
        "        angle = rng.uniform(0, 2 * np.pi)\n",
        "        x = r * np.sin(angle)\n",
        "        y = r * np.cos(angle)\n",
        "        noise_x = rng.uniform(-radius, radius) * noise\n",
        "        noise_y = rng.uniform(-radius, radius) * noise\n",
        "        label = get_circle_label(Example2D(x + noise_x, y + noise_y, None), Example2D(0, 0, None))\n",
        "        points.append(Example2D(x, y, label))\n",
        "\n",
        "    # Generate negative points outside the circle.\n",
        "    for _ in range(num_samples // 2):\n",
        "        r = rng.uniform(radius * 0.7, radius)\n",
        "        angle = rng.uniform(0, 2 * np.pi)\n",
        "        x = r * np.sin(angle)\n",
        "        y = r * np.cos(angle)\n",
        "        noise_x = rng.uniform(-radius, radius) * noise\n",
        "        noise_y = rng.uniform(-radius, radius) * noise\n",
        "        label = get_circle_label(Example2D(x + noise_x, y + noise_y, None), Example2D(0, 0, None))\n",
        "        points.append(Example2D(x, y, label))\n",
        "    return points\n",
        "\n",
        "class DataGeneratorFactory:\n",
        "    def __init__(self):\n",
        "        self._factory_methods = {}\n",
        "\n",
        "    def register(self, data_type, factory_method):\n",
        "        self._factory_methods[data_type] = factory_method\n",
        "\n",
        "    def create_generator(self, data_type, num_samples, noise):\n",
        "        factory_method = self._factory_methods.get(data_type)\n",
        "        if factory_method is None:\n",
        "            raise ValueError(f\"No factory method registered for data type: {data_type}\")\n",
        "        return factory_method(num_samples, noise)\n",
        "\n",
        "df = DataGeneratorFactory()\n",
        "df.register(\"gauss\", classify_two_gauss_data)\n",
        "df.register(\"circle\", classify_circle_data)\n",
        "df.register(\"xor\", classify_xor_data)\n",
        "df.register(\"spiral\", classify_spiral_data)\n"
      ],
      "metadata": {
        "id": "WetLceMaXe_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import random_split\n",
        "!pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/torch_stable.html\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "oXHqr8bsXmEu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ff53e4-054d-4fde-e916-6b3e0e2a6532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, activation = 'relu'):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        # Определение функции активации\n",
        "        if activation == 'sigmoid':\n",
        "            activation = nn.Sigmoid()\n",
        "        elif activation == 'tanh':\n",
        "            activation = nn.Tanh()\n",
        "        elif activation == 'relu':\n",
        "            activation = nn.ReLU()\n",
        "        else:\n",
        "            raise ValueError(\"Неизвестная функция активации\")\n",
        "\n",
        "        # Список слоёв\n",
        "        layers = []\n",
        "\n",
        "        # Добавление входного слоя\n",
        "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
        "        layers.append(activation)\n",
        "\n",
        "        # Добавление скрытых слоёв\n",
        "        for i in range(len(hidden_sizes) - 1):\n",
        "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
        "            layers.append(activation)\n",
        "\n",
        "        # Добавление выходного слоя\n",
        "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
        "        layers.append(nn.Sigmoid())\n",
        "\n",
        "        # Создание последовательности слоёв\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # x.to(device)\n",
        "      return self.layers(x)"
      ],
      "metadata": {
        "id": "u6phb5b9YVHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, loss_function, epochs = 1000, verbose = False, test_data = None):\n",
        "  for epoch in range(epochs):\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        # Forward pass\n",
        "        outputs = model(inputs).squeeze()\n",
        "        loss = loss_function(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "        loss.backward()       # Calculate gradients for the entire batch\n",
        "\n",
        "        # Update weights using the accumulated gradients\n",
        "        optimizer.step()\n",
        "\n",
        "    if verbose and epoch % 100 == 0:\n",
        "      model.eval()\n",
        "      with torch.inference_mode():\n",
        "        test_input, test_labels = test_data\n",
        "        test_output = model(test_input).squeeze()\n",
        "        test_loss = loss_function(test_output, test_labels)\n",
        "        print(f\"epoch={epoch}, train_loss={loss}, test_loss={test_loss}\")"
      ],
      "metadata": {
        "id": "1SxoB03tjlAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have test_loader with test data\n",
        "def compute_confusion_matrix(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            predicted_labels = (outputs > 0.5).float()  # Threshold predictions\n",
        "            y_true.extend(labels.cpu())\n",
        "            y_pred.extend(predicted_labels.cpu())\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    return cm"
      ],
      "metadata": {
        "id": "eSrnZStjyVBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size = 640\n",
        "ratio = 0.8\n",
        "noise = 0.1\n",
        "datatype = \"circle\"\n",
        "\n",
        "input_size = 2\n",
        "hidden_sizes = [4, 4, 4]\n",
        "output_size = 1\n",
        "activation = 'relu'\n",
        "\n",
        "learning_rate = 0.03\n",
        "epochs = 1000\n",
        "batch = 64"
      ],
      "metadata": {
        "id": "odt2ApfjnKfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = df.create_generator(datatype, dataset_size, noise)\n",
        "data = torch.from_numpy(np.array([[p.x,p.y] for p in d])).type(torch.FloatTensor).to(device)\n",
        "labels = torch.from_numpy(np.array([p.label for p in d])).type(torch.FloatTensor).to(device)\n",
        "dataset = TensorDataset(data,labels)\n",
        "\n",
        "generator = torch.Generator().manual_seed(42)\n",
        "train_data, test_data = random_split(dataset, [ratio, 1-ratio], generator=generator)\n",
        "train_loader = DataLoader(train_data, batch_size = batch, shuffle = True, drop_last=True)\n",
        "test_loader = DataLoader(test_data, batch_size = batch, shuffle = True, drop_last=False)"
      ],
      "metadata": {
        "id": "rIyTQqO4oEKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(input_size, hidden_sizes, output_size)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "dYo_zeuVrlFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa3fa95-4d01-4de2-dea7-230d3deab6fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=2, out_features=4, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=4, out_features=4, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=4, out_features=4, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=4, out_features=1, bias=True)\n",
              "    (7): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "loss_function = torch.nn.BCELoss()"
      ],
      "metadata": {
        "id": "bz3VEfB7t-Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, train_loader, optimizer, loss_function, epochs = epochs, verbose = True, test_data = test_data[0:-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxdPXsHAuyRt",
        "outputId": "6d4a546a-e760-4b3d-e7cf-8580999280e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=0, train_loss=0.6976854801177979, test_loss=0.6948580741882324\n",
            "epoch=100, train_loss=0.27213385701179504, test_loss=0.2956138253211975\n",
            "epoch=200, train_loss=0.07930422574281693, test_loss=0.09684564918279648\n",
            "epoch=300, train_loss=0.09133585542440414, test_loss=0.07424002140760422\n",
            "epoch=400, train_loss=0.0470026358962059, test_loss=0.06488482654094696\n",
            "epoch=500, train_loss=0.04602641612291336, test_loss=0.059617992490530014\n",
            "epoch=600, train_loss=0.05175651237368584, test_loss=0.056550223380327225\n",
            "epoch=700, train_loss=0.06159396469593048, test_loss=0.055741239339113235\n",
            "epoch=800, train_loss=0.055971525609493256, test_loss=0.055209673941135406\n",
            "epoch=900, train_loss=0.05065659433603287, test_loss=0.0556144118309021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm = compute_confusion_matrix(model, test_loader)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asmOOWue3XyC",
        "outputId": "a2c7e57a-e9d5-4ef0-a865-51fe080a8c29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[59  2]\n",
            " [ 1 65]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K Fold Cross Validation"
      ],
      "metadata": {
        "id": "Ya070kyT5v_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "id": "Hq1c6RUt5vQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k=10\n",
        "\n",
        "cv_dataset_size = 640\n",
        "cv_ratio = 0.8\n",
        "cv_noise = 0.1\n",
        "cv_datatype = \"circle\"\n",
        "cv_shuffle=True\n",
        "\n",
        "cv_input_size = 2\n",
        "cv_hidden_sizes = [4, 4, 4]\n",
        "cv_output_size = 1\n",
        "\n",
        "cv_activation = 'relu'\n",
        "cv_loss_function = torch.nn.BCELoss()\n",
        "\n",
        "cv_learning_rate = 0.03\n",
        "cv_epochs = 1000\n",
        "cv_batch = 64"
      ],
      "metadata": {
        "id": "v80vaKW_5-6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=k, shuffle=True)"
      ],
      "metadata": {
        "id": "zReweqPA6nhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss(model, data_loader, loss_function):\n",
        "  loss = 0.0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in data_loader:\n",
        "          outputs = model(inputs).squeeze()\n",
        "          loss = loss + loss_function(outputs, labels)\n",
        "  return loss/(len(data_loader.dataset)/data_loader.batch_size)"
      ],
      "metadata": {
        "id": "1t1vgHmPCZil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_fold(fold, data, train_indices, val_indices):\n",
        "  model = MLP(cv_input_size, cv_hidden_sizes, cv_output_size)\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=cv_learning_rate)\n",
        "\n",
        "  train_fold = torch.utils.data.Subset(data, train_indices)\n",
        "  val_fold = torch.utils.data.Subset(data, val_indices)\n",
        "\n",
        "  train_loader = DataLoader(train_fold, batch_size=cv_batch, shuffle=True)\n",
        "  val_loader = DataLoader(val_fold, batch_size=cv_batch)\n",
        "\n",
        "  train(model, train_loader, optimizer, cv_loss_function, cv_epochs)\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    train_loss = calc_loss(model, train_loader, cv_loss_function)\n",
        "    test_loss = calc_loss(model, val_loader, cv_loss_function)\n",
        "    return fold, train_loss, test_loss"
      ],
      "metadata": {
        "id": "3a50LkWlJTXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validation(kf, data):\n",
        "  for fold, (train_indices, val_indices) in enumerate(kf.split(data)):\n",
        "    fold, tr_l, t_l = eval_fold(fold, data, train_indices, val_indices)\n",
        "    print(f\"fold={fold+1}, train loss={tr_l}, test loss={t_l}\")\n"
      ],
      "metadata": {
        "id": "-yjtGoJx6ooO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_d = df.create_generator(cv_datatype, cv_dataset_size, cv_noise)\n",
        "cv_data = torch.from_numpy(np.array([[p.x,p.y] for p in cv_d])).type(torch.FloatTensor).to(device)\n",
        "cv_labels = torch.from_numpy(np.array([p.label for p in cv_d])).type(torch.FloatTensor).to(device)\n",
        "cv_dataset = TensorDataset(cv_data,cv_labels)"
      ],
      "metadata": {
        "id": "ciM3dxvk-x9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for activ in ['relu','sigmoid','tanh']:\n",
        "  for hidden_layers in [[4],[4,4],[4,4,4]]:\n",
        "    print(\"#######Cross validation########\\n\")\n",
        "    print(f\"activation function={activ}, hidden_layers={hidden_layers}\")\n",
        "    cv_hidden_sizes = hidden_layers\n",
        "    cv_activation = activ\n",
        "    cross_validation(kf, cv_dataset)\n",
        "    print(\"#############################\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-AguGWbVWJ8",
        "outputId": "042647ea-0c5e-4fdd-faa4-d64ad1564b0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#######Cross validation########\n",
            "\n",
            "activation function=relu, hidden_layers=[4]\n",
            "fold=1, train loss=0.061938121914863586, test loss=0.032712168991565704\n",
            "fold=2, train loss=0.049510132521390915, test loss=0.0670933723449707\n",
            "fold=3, train loss=0.047126006335020065, test loss=0.07030381262302399\n",
            "fold=4, train loss=0.052470069378614426, test loss=0.0757589340209961\n",
            "fold=5, train loss=0.06354282051324844, test loss=0.04404450207948685\n",
            "fold=6, train loss=0.06327550858259201, test loss=0.10278534889221191\n",
            "fold=7, train loss=0.05530061572790146, test loss=0.03341912850737572\n",
            "fold=8, train loss=0.07216566801071167, test loss=0.03868803009390831\n",
            "fold=9, train loss=0.05091844126582146, test loss=0.08520875871181488\n",
            "fold=10, train loss=0.050578195601701736, test loss=0.06768236309289932\n",
            "#############################\n",
            "\n",
            "#######Cross validation########\n",
            "\n",
            "activation function=relu, hidden_layers=[4, 4]\n",
            "fold=1, train loss=0.04637281969189644, test loss=0.07471950352191925\n",
            "fold=2, train loss=0.05181819200515747, test loss=0.03268270939588547\n",
            "fold=3, train loss=0.04177186265587807, test loss=0.09343992173671722\n",
            "fold=4, train loss=0.04724962264299393, test loss=0.03245605155825615\n",
            "fold=5, train loss=0.04197057709097862, test loss=0.08524176478385925\n",
            "fold=6, train loss=0.0550776869058609, test loss=0.04684652388095856\n",
            "fold=7, train loss=0.0466378889977932, test loss=0.07131344825029373\n",
            "fold=8, train loss=0.04751024767756462, test loss=0.08539015799760818\n",
            "fold=9, train loss=0.0501687228679657, test loss=0.05395772308111191\n",
            "fold=10, train loss=0.050783220678567886, test loss=0.0638117641210556\n",
            "#############################\n",
            "\n",
            "#######Cross validation########\n",
            "\n",
            "activation function=relu, hidden_layers=[4, 4, 4]\n",
            "fold=1, train loss=0.038582563400268555, test loss=0.14682330191135406\n",
            "fold=2, train loss=0.05619778111577034, test loss=0.03330474719405174\n",
            "fold=3, train loss=0.04209086671471596, test loss=0.1014397144317627\n",
            "fold=4, train loss=0.05071636661887169, test loss=0.06064063310623169\n",
            "fold=5, train loss=0.048919837921857834, test loss=0.08579357713460922\n",
            "fold=6, train loss=0.04880358651280403, test loss=0.05046030133962631\n",
            "fold=7, train loss=0.04883314669132233, test loss=0.015696410089731216\n",
            "fold=8, train loss=0.05629042163491249, test loss=0.0068982625380158424\n",
            "fold=9, train loss=0.04393292963504791, test loss=0.06935713440179825\n",
            "fold=10, train loss=0.0429922379553318, test loss=0.11475957930088043\n",
            "#############################\n",
            "\n",
            "#######Cross validation########\n",
            "\n",
            "activation function=sigmoid, hidden_layers=[4]\n",
            "fold=1, train loss=0.06120442599058151, test loss=0.033037591725587845\n",
            "fold=2, train loss=0.4139784872531891, test loss=0.32335418462753296\n",
            "fold=3, train loss=0.05804060772061348, test loss=0.06883373856544495\n",
            "fold=4, train loss=0.05386362969875336, test loss=0.04015272855758667\n",
            "fold=5, train loss=0.049223557114601135, test loss=0.06872212886810303\n",
            "fold=6, train loss=0.05604306235909462, test loss=0.08771584928035736\n",
            "fold=7, train loss=0.06337745487689972, test loss=0.02938687615096569\n",
            "fold=8, train loss=0.0631006509065628, test loss=0.11301171779632568\n",
            "fold=9, train loss=0.0607135184109211, test loss=0.04493892937898636\n",
            "fold=10, train loss=0.05692972242832184, test loss=0.08320176601409912\n",
            "#############################\n",
            "\n",
            "#######Cross validation########\n",
            "\n",
            "activation function=sigmoid, hidden_layers=[4, 4]\n",
            "fold=1, train loss=0.041532158851623535, test loss=0.10140295326709747\n",
            "fold=2, train loss=0.04991160333156586, test loss=0.014411959797143936\n",
            "fold=3, train loss=0.0520985871553421, test loss=0.031305376440286636\n",
            "fold=4, train loss=0.05112624540925026, test loss=0.08529554307460785\n",
            "fold=5, train loss=0.05037635937333107, test loss=0.028411298990249634\n",
            "fold=6, train loss=0.052543286234140396, test loss=0.02963157370686531\n",
            "fold=7, train loss=0.0522150881588459, test loss=0.04861950874328613\n",
            "fold=8, train loss=0.046014633029699326, test loss=0.07903715968132019\n",
            "fold=9, train loss=0.04413898289203644, test loss=0.10750693082809448\n",
            "fold=10, train loss=0.04103844240307808, test loss=0.12544624507427216\n",
            "#############################\n",
            "\n",
            "#######Cross validation########\n",
            "\n",
            "activation function=sigmoid, hidden_layers=[4, 4, 4]\n",
            "fold=1, train loss=0.049316324293613434, test loss=0.016394617035984993\n",
            "fold=2, train loss=0.041502002626657486, test loss=0.12717106938362122\n",
            "fold=3, train loss=0.042662013322114944, test loss=0.10083401203155518\n",
            "fold=4, train loss=0.04358890280127525, test loss=0.1036643534898758\n",
            "fold=5, train loss=0.04736218973994255, test loss=0.023548820987343788\n",
            "fold=6, train loss=0.04568003490567207, test loss=0.053554512560367584\n",
            "fold=7, train loss=0.042691558599472046, test loss=0.0804760605096817\n",
            "fold=8, train loss=0.04189681261777878, test loss=0.07193788886070251\n",
            "fold=9, train loss=0.06077497452497482, test loss=0.0647311881184578\n",
            "fold=10, train loss=0.04574521258473396, test loss=0.042614080011844635\n",
            "#############################\n",
            "\n",
            "#######Cross validation########\n",
            "\n",
            "activation function=tanh, hidden_layers=[4]\n",
            "fold=1, train loss=0.05735822021961212, test loss=0.031849272549152374\n",
            "fold=2, train loss=0.06060684472322464, test loss=0.07906538993120193\n",
            "fold=3, train loss=0.35565483570098877, test loss=0.4414968490600586\n",
            "fold=4, train loss=0.07139775902032852, test loss=0.0196441151201725\n",
            "fold=5, train loss=0.057793863117694855, test loss=0.09647959470748901\n",
            "fold=6, train loss=0.037092868238687515, test loss=0.179653137922287\n",
            "fold=7, train loss=0.062234751880168915, test loss=0.031497322022914886\n",
            "fold=8, train loss=0.0499696210026741, test loss=0.04805334284901619\n",
            "fold=9, train loss=0.05542701482772827, test loss=0.10657373815774918\n",
            "fold=10, train loss=0.06355302780866623, test loss=0.09744337946176529\n",
            "#############################\n",
            "\n",
            "#######Cross validation########\n",
            "\n",
            "activation function=tanh, hidden_layers=[4, 4]\n",
            "fold=1, train loss=0.04526253417134285, test loss=0.1510665863752365\n",
            "fold=2, train loss=0.05407264828681946, test loss=0.05250776559114456\n",
            "fold=3, train loss=0.052001792937517166, test loss=0.06621548533439636\n",
            "fold=4, train loss=0.04665418341755867, test loss=0.06147097796201706\n",
            "fold=5, train loss=0.04562045633792877, test loss=0.0696239173412323\n",
            "fold=6, train loss=0.04407883062958717, test loss=0.06765343248844147\n",
            "fold=7, train loss=0.039475370198488235, test loss=0.12483766674995422\n",
            "fold=8, train loss=0.0507468581199646, test loss=0.03630495071411133\n",
            "fold=9, train loss=0.05241463705897331, test loss=0.032023534178733826\n",
            "fold=10, train loss=0.049825627356767654, test loss=0.05916319787502289\n",
            "#############################\n",
            "\n",
            "#######Cross validation########\n",
            "\n",
            "activation function=tanh, hidden_layers=[4, 4, 4]\n",
            "fold=1, train loss=0.048552095890045166, test loss=0.0197621900588274\n",
            "fold=2, train loss=0.04191494733095169, test loss=0.14714765548706055\n",
            "fold=3, train loss=0.04897855222225189, test loss=0.08834874629974365\n",
            "fold=4, train loss=0.048023391515016556, test loss=0.028164532035589218\n",
            "fold=5, train loss=0.04774867370724678, test loss=0.040228333324193954\n",
            "fold=6, train loss=0.04695101082324982, test loss=0.06434657424688339\n",
            "fold=7, train loss=0.05175372213125229, test loss=0.046586669981479645\n",
            "fold=8, train loss=0.03533235564827919, test loss=0.21180686354637146\n",
            "fold=9, train loss=0.04788963496685028, test loss=0.016798589378595352\n",
            "fold=10, train loss=0.04524747282266617, test loss=0.04939408227801323\n",
            "#############################\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"#######Cross validation########\n",
        "\n",
        "activation function=relu, hidden_layers=[4]\n",
        "fold=1, train loss=0.061938121914863586, test loss=0.032712168991565704\n",
        "fold=2, train loss=0.049510132521390915, test loss=0.0670933723449707\n",
        "fold=3, train loss=0.047126006335020065, test loss=0.07030381262302399\n",
        "fold=4, train loss=0.052470069378614426, test loss=0.0757589340209961\n",
        "fold=5, train loss=0.06354282051324844, test loss=0.04404450207948685\n",
        "fold=6, train loss=0.06327550858259201, test loss=0.10278534889221191\n",
        "fold=7, train loss=0.05530061572790146, test loss=0.03341912850737572\n",
        "fold=8, train loss=0.07216566801071167, test loss=0.03868803009390831\n",
        "fold=9, train loss=0.05091844126582146, test loss=0.08520875871181488\n",
        "fold=10, train loss=0.050578195601701736, test loss=0.06768236309289932\n",
        "#############################\n",
        "\n",
        "#######Cross validation########\n",
        "\n",
        "activation function=relu, hidden_layers=[4, 4]\n",
        "fold=1, train loss=0.04637281969189644, test loss=0.07471950352191925\n",
        "fold=2, train loss=0.05181819200515747, test loss=0.03268270939588547\n",
        "fold=3, train loss=0.04177186265587807, test loss=0.09343992173671722\n",
        "fold=4, train loss=0.04724962264299393, test loss=0.03245605155825615\n",
        "fold=5, train loss=0.04197057709097862, test loss=0.08524176478385925\n",
        "fold=6, train loss=0.0550776869058609, test loss=0.04684652388095856\n",
        "fold=7, train loss=0.0466378889977932, test loss=0.07131344825029373\n",
        "fold=8, train loss=0.04751024767756462, test loss=0.08539015799760818\n",
        "fold=9, train loss=0.0501687228679657, test loss=0.05395772308111191\n",
        "fold=10, train loss=0.050783220678567886, test loss=0.0638117641210556\n",
        "#############################\n",
        "\n",
        "#######Cross validation########\n",
        "\n",
        "activation function=relu, hidden_layers=[4, 4, 4]\n",
        "fold=1, train loss=0.038582563400268555, test loss=0.14682330191135406\n",
        "fold=2, train loss=0.05619778111577034, test loss=0.03330474719405174\n",
        "fold=3, train loss=0.04209086671471596, test loss=0.1014397144317627\n",
        "fold=4, train loss=0.05071636661887169, test loss=0.06064063310623169\n",
        "fold=5, train loss=0.048919837921857834, test loss=0.08579357713460922\n",
        "fold=6, train loss=0.04880358651280403, test loss=0.05046030133962631\n",
        "fold=7, train loss=0.04883314669132233, test loss=0.015696410089731216\n",
        "fold=8, train loss=0.05629042163491249, test loss=0.0068982625380158424\n",
        "fold=9, train loss=0.04393292963504791, test loss=0.06935713440179825\n",
        "fold=10, train loss=0.0429922379553318, test loss=0.11475957930088043\n",
        "#############################\n",
        "\n",
        "#######Cross validation########\n",
        "\n",
        "activation function=sigmoid, hidden_layers=[4]\n",
        "fold=1, train loss=0.06120442599058151, test loss=0.033037591725587845\n",
        "fold=2, train loss=0.4139784872531891, test loss=0.32335418462753296\n",
        "fold=3, train loss=0.05804060772061348, test loss=0.06883373856544495\n",
        "fold=4, train loss=0.05386362969875336, test loss=0.04015272855758667\n",
        "fold=5, train loss=0.049223557114601135, test loss=0.06872212886810303\n",
        "fold=6, train loss=0.05604306235909462, test loss=0.08771584928035736\n",
        "fold=7, train loss=0.06337745487689972, test loss=0.02938687615096569\n",
        "fold=8, train loss=0.0631006509065628, test loss=0.11301171779632568\n",
        "fold=9, train loss=0.0607135184109211, test loss=0.04493892937898636\n",
        "fold=10, train loss=0.05692972242832184, test loss=0.08320176601409912\n",
        "#############################\n",
        "\n",
        "#######Cross validation########\n",
        "\n",
        "activation function=sigmoid, hidden_layers=[4, 4]\n",
        "fold=1, train loss=0.041532158851623535, test loss=0.10140295326709747\n",
        "fold=2, train loss=0.04991160333156586, test loss=0.014411959797143936\n",
        "fold=3, train loss=0.0520985871553421, test loss=0.031305376440286636\n",
        "fold=4, train loss=0.05112624540925026, test loss=0.08529554307460785\n",
        "fold=5, train loss=0.05037635937333107, test loss=0.028411298990249634\n",
        "fold=6, train loss=0.052543286234140396, test loss=0.02963157370686531\n",
        "fold=7, train loss=0.0522150881588459, test loss=0.04861950874328613\n",
        "fold=8, train loss=0.046014633029699326, test loss=0.07903715968132019\n",
        "fold=9, train loss=0.04413898289203644, test loss=0.10750693082809448\n",
        "fold=10, train loss=0.04103844240307808, test loss=0.12544624507427216\n",
        "#############################\n",
        "\n",
        "#######Cross validation########\n",
        "\n",
        "activation function=sigmoid, hidden_layers=[4, 4, 4]\n",
        "fold=1, train loss=0.049316324293613434, test loss=0.016394617035984993\n",
        "fold=2, train loss=0.041502002626657486, test loss=0.12717106938362122\n",
        "fold=3, train loss=0.042662013322114944, test loss=0.10083401203155518\n",
        "fold=4, train loss=0.04358890280127525, test loss=0.1036643534898758\n",
        "fold=5, train loss=0.04736218973994255, test loss=0.023548820987343788\n",
        "fold=6, train loss=0.04568003490567207, test loss=0.053554512560367584\n",
        "fold=7, train loss=0.042691558599472046, test loss=0.0804760605096817\n",
        "fold=8, train loss=0.04189681261777878, test loss=0.07193788886070251\n",
        "fold=9, train loss=0.06077497452497482, test loss=0.0647311881184578\n",
        "fold=10, train loss=0.04574521258473396, test loss=0.042614080011844635\n",
        "#############################\n",
        "\n",
        "#######Cross validation########\n",
        "\n",
        "activation function=tanh, hidden_layers=[4]\n",
        "fold=1, train loss=0.05735822021961212, test loss=0.031849272549152374\n",
        "fold=2, train loss=0.06060684472322464, test loss=0.07906538993120193\n",
        "fold=3, train loss=0.35565483570098877, test loss=0.4414968490600586\n",
        "fold=4, train loss=0.07139775902032852, test loss=0.0196441151201725\n",
        "fold=5, train loss=0.057793863117694855, test loss=0.09647959470748901\n",
        "fold=6, train loss=0.037092868238687515, test loss=0.179653137922287\n",
        "fold=7, train loss=0.062234751880168915, test loss=0.031497322022914886\n",
        "fold=8, train loss=0.0499696210026741, test loss=0.04805334284901619\n",
        "fold=9, train loss=0.05542701482772827, test loss=0.10657373815774918\n",
        "fold=10, train loss=0.06355302780866623, test loss=0.09744337946176529\n",
        "#############################\n",
        "\n",
        "#######Cross validation########\n",
        "\n",
        "activation function=tanh, hidden_layers=[4, 4]\n",
        "fold=1, train loss=0.04526253417134285, test loss=0.1510665863752365\n",
        "fold=2, train loss=0.05407264828681946, test loss=0.05250776559114456\n",
        "fold=3, train loss=0.052001792937517166, test loss=0.06621548533439636\n",
        "fold=4, train loss=0.04665418341755867, test loss=0.06147097796201706\n",
        "fold=5, train loss=0.04562045633792877, test loss=0.0696239173412323\n",
        "fold=6, train loss=0.04407883062958717, test loss=0.06765343248844147\n",
        "fold=7, train loss=0.039475370198488235, test loss=0.12483766674995422\n",
        "fold=8, train loss=0.0507468581199646, test loss=0.03630495071411133\n",
        "fold=9, train loss=0.05241463705897331, test loss=0.032023534178733826\n",
        "fold=10, train loss=0.049825627356767654, test loss=0.05916319787502289\n",
        "#############################\n",
        "\n",
        "#######Cross validation########\n",
        "\n",
        "activation function=tanh, hidden_layers=[4, 4, 4]\n",
        "fold=1, train loss=0.048552095890045166, test loss=0.0197621900588274\n",
        "fold=2, train loss=0.04191494733095169, test loss=0.14714765548706055\n",
        "fold=3, train loss=0.04897855222225189, test loss=0.08834874629974365\n",
        "fold=4, train loss=0.048023391515016556, test loss=0.028164532035589218\n",
        "fold=5, train loss=0.04774867370724678, test loss=0.040228333324193954\n",
        "fold=6, train loss=0.04695101082324982, test loss=0.06434657424688339\n",
        "fold=7, train loss=0.05175372213125229, test loss=0.046586669981479645\n",
        "fold=8, train loss=0.03533235564827919, test loss=0.21180686354637146\n",
        "fold=9, train loss=0.04788963496685028, test loss=0.016798589378595352\n",
        "fold=10, train loss=0.04524747282266617, test loss=0.04939408227801323\n",
        "#############################\n",
        "\"\"\".split(\"#############################\")\n",
        "\n",
        "for part in text:\n",
        "  lines = part.split(\"\\n\")\n",
        "  loss = 0.0\n",
        "  total = 0\n",
        "  for line in lines:\n",
        "    if line.find(\"activation function\") != -1:\n",
        "      print(line)\n",
        "    elif line.find(\"test loss\") != -1:\n",
        "      loss+=float(line.split(\"test loss=\")[1])\n",
        "      total+=1\n",
        "  if total != 0:\n",
        "    print(f\"Avg loss={loss/total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgTRcPpffx6f",
        "outputId": "7b78cd52-5ff8-475a-eb83-9d282c339886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "activation function=relu, hidden_layers=[4]\n",
            "Avg loss=0.06176964193582535\n",
            "activation function=relu, hidden_layers=[4, 4]\n",
            "Avg loss=0.06398595683276653\n",
            "activation function=relu, hidden_layers=[4, 4, 4]\n",
            "Avg loss=0.06851736614480615\n",
            "activation function=sigmoid, hidden_layers=[4]\n",
            "Avg loss=0.08923555109649897\n",
            "activation function=sigmoid, hidden_layers=[4, 4]\n",
            "Avg loss=0.06510685496032238\n",
            "activation function=sigmoid, hidden_layers=[4, 4, 4]\n",
            "Avg loss=0.06849266029894352\n",
            "activation function=tanh, hidden_layers=[4]\n",
            "Avg loss=0.1131756141781807\n",
            "activation function=tanh, hidden_layers=[4, 4]\n",
            "Avg loss=0.07208675146102905\n",
            "activation function=tanh, hidden_layers=[4, 4, 4]\n",
            "Avg loss=0.07125842366367578\n"
          ]
        }
      ]
    }
  ]
}